<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="google-site-verification" content="SSjHVyWmhmgx2MqYFKLPKg9LJfgB-RjyW42N5XOiruA" />
    <meta name="viewport" content="width=device-width">
    <style type="text/css">
        .container {
            zoom: 1;
            margin-left: auto;
            margin-right: auto;
            vertical-align: middle;
            text-align: left;
            width: 100%;
            max-width: 800px;
        }

        .content {
            margin-bottom: -10px;
            display: inline-block;
            display*: inline;
            width: 100%;
        }

        a {
            color: #1772d0;
            text-decoration: none;
            transition: 0.3s all cubic-bezier(0.42, 0, 0.57, 1.96);
        }

        a:focus,
        a:hover {
            color: rgb(252, 76, 122);
            border-color: rgb(252, 76, 122);
        }
        
        * {
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
        }

    </style>
    <title>Yong Li's Homepage</title>
    <link rel="icon" href="/my_picture.jpg">
</head>

<body>
    <div class="container">
        <table width="100%" align="center" border="0">
            <tr>
                <td width="65%" valign="middle">
                    <h1 style="margin-top:20px;margin-bottom:36px;">Yong Li (李勇)</h1>
                    <p style="font-size:20px; color:#000">
                        School of Computer Science and Engineering,
                        Nanjing University of Science and Technology, Nanjing, China.
                    </p>
                    <p>
                        Email: yong [dot] li [at] njust [dot] edu [dot] cn
                    </p>
                    <p>
                        <a href="https://scholar.google.com/citations?user=HRBTJYYAAAAJ&hl=zh-CN">[Google Scholar]</a>
                        &nbsp;
                        <a href="https://github.com/mysee1989">[Github]</a>
                        &nbsp;
                        <!-- <b>
                            <font color="red"> Actively looking for PhD position!</font>
                        </b> -->
                    </p>
                </td>
                <td width="35%" align="center">
                    <img src="/my_picture.jpg" width="80%" />
                </td>
            </tr>
        </table>

        <h2 id="Biography">Biography</h2>
        <hr style="margin-top:-16px;margin-bottom:10px;" />

        <p>
            Received a Ph.D. degree in computer science from <a href="http://www.ict.ac.cn">Institute of Computing Technology (ICT)</a>, Chinese Academy of Science (CAS), Beijing, in Jun. 2020, advised by professor <a href="https://vipl.ict.ac.cn/people/~sgshan">Shiguang Shan</a>. I was also fortunate to cooperate tightly with <a href="https://dualplus.github.io/">Jiabei Zeng</a> during my four year phd candidate career. 
             <br />
             <dt>
             I focus on affective-computing-related machine learning methodologies, e.g., <font size="3"><b>learning with small data regime</b></font> and <font size="3"><b>generalize to unseen identities</b></font>, <font size="3"><b>generative modelling</b></font>, <font size="3"><b>multimodal understanding</b></font>. 
            </dt>
            
            <br />
             <dt>
             The ultimate goal of my research is to empower machines with emotion and reasoning intelligence.
            </dt>
            
        </p>

        <h2 id="education">Education</h2>
        <hr style="margin-top:-16px;margin-bottom:10px;" />

        <dl>
            <!-- <dt>
                <img align="left" width="80" height="65" hspace="10" src="assets/imgs/ICT-logo.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Chinese Academy of Sciences,Beijing,China</b></font>
            </dt>
            <dt>
                <i> Sep.2016 – Jun.2020</i>
            </dt>
            <dt>
                Ph.D. student, Computer Sciences
            </dt>

            <br />

            <!-- <dt>
                <img align="left" width="80" height="80" hspace="10" src="assets/imgs/xdu-logo.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Zhengzhou University</b></font>
            </dt>
            <dt>
                <i> Sep.2012 – Jul.2015</i>
            </dt>
            <dt>
                M.S., Computer Software and Theory
            </dt>
            <br />

            <dt>
                <font size="4"><b>Zhengzhou University</b></font>
            </dt>
            <dt>
                <i>Sep.2008 – Jul.2012</i>
            </dt>
            <dt>
                B.S., Computer Science
            </dt>

        </dl>

        <h2 id="publications">Selected Publications</h2>
        <hr style="margin-top:-16px;margin-bottom:10px;" />

        <dl>
            
            <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/BMVC2019.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Learning Representations for Facial Actions from Unlabeled Videos</b></font>
            </dt>
            <dt>
               <strong>Yong Li</strong>, Jiabei Zeng, Shiguang Shan
            </dt>
            <dt>
                <i>IEEE Transactions on Pattern Analysis and Machine Intelligence(<b>TPAMI</b>), 2020.</i>
            </dt>
            <dt>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145674">[PDF]
                </a>
            </dt>
            <br />
            
             <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/DoubleAnchor.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Decoupled Multimodal Distilling for Emotion Recognition</b></font>
            </dt>
            <dt>
               <strong>Yong Li</strong>, Yuanzhi Wang, Zhen Cui
            </dt>
            <dt>
                <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>),2023 <font color="red"><b>(Highlight, top 10% of accepted papers, 2.5% of submissions)</b></font>.</i>
            </dt>
            <dt>
                <a href="https://arxiv.org/abs/2303.13802">[PDF]</a>
                 &nbsp;
                <a href="https://github.com/mdswyz/DMD">[Code]</a>
            </dt>
            <br />
            
            <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/DynamicRCNN.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Contrastive Learning of Person-independent Representations for Facial Action Unit Detection</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>, Shiguang Shan
            </dt>
            <dt>
                 <i>IEEE Transactions on Image Processing (<b>TIP</b>), 2023.</i>
            </dt>
             <dt>
                <i>Minor Revision</i>
            </dt>
            </dt>
            <br />
        
        <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/DynamicRCNN.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Graph Jigsaw Learning for Cartoon Face Recognition</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>, Yufei Sun, Zhen Cui, Shiguang Shan, Jian Yang
            </dt>
            <dt>
                 <i>IEEE Transactions on Image Processing  (<b>TIP</b>), 2022.</i>
            </dt>
            <dt>
                <a href="https://arxiv.org/abs/2107.06532">[PDF]</a>
                &nbsp;
                <a href="https://github.com/mysee1989/GraphJigsaw">[Code]</a>
            </dt>
            </dt>
            <br />
    
    <dt>
                <font size="4"><b>3D3M: 3D Modulated Morphable Model for Monocular Face Reconstruction</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>,  Shiguang Shan
            </dt>
            <dt>
                <i>IEEE Transactions on Multimedia (TMM), 2022.</i>
            </dt>
            <dt>
                <a href="https://ieeexplore.ieee.org/document/9913696/">[PDF]</a>
                &nbsp;
            </dt>
            <br />
            
           
            <dt>
                <font size="4"><b>Learning Fair Face Representation With Progressive Cross Transformer</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>, Lingjie Lao, Zhen Cui, Shiguang Shan, Jian Yang
            </dt>
            <dt>
                <i>Arxiv 2021, under review.</i>
            </dt>
            <dt>
                <a href="https://arxiv.org/abs/2108.04983">[PDF]</a>
                &nbsp;
            </dt>

            <br />
    
    <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/DynamicRCNN.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Meta Auxiliary Learning for Facial Action Unit Detection</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>,  Shiguang Shan
            </dt>
            <dt>
                <i>IEEE Transactions on Affective Computing (TAC), 2021.</i>
            </dt>
            <dt>
                <a href="https://arxiv.org/abs/2105.06620">[PDF]</a>
                &nbsp;
            </dt>
             <br />
            
             <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/DoubleAnchor.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Self-supervised representation learning from videos for facial action unit detection</b></font>
            </dt>
            <dt>
               <strong>Yong Li</strong>, Jiabei Zeng, Shiguang Shan, Xilin Chen
            </dt>
            <dt>
                <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>),2019 <font color="red"><b>(Oral)</b></font>.</i>
            </dt>
            <dt>
                <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Self-Supervised_Representation_Learning_From_Videos_for_Facial_Action_Unit_Detection_CVPR_2019_paper.pdf">[PDF]</a>
                 &nbsp;
                <a href="https://github.com/mysee1989/TCAE">[Code]</a>
            </dt>
            <br />
            
            
            <!-- <dt>
                <img align="left" width="200" height="100" hspace="10" src="assets/imgs/DynamicRCNN.jpg" />
            </dt> -->
            <dt>
                <font size="4"><b>Occlusion aware facial expression recognition using cnn with attention mechanism</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>, Jiabei Zeng, Shiguang Shan, Xilin Chen
            </dt>
            <dt>
                <i>IEEE Transactions on Image Processing 28 (5), 2439-2450 (<b>TIP</b>), 2020.</i>
            </dt>
            <dt>
                <a href="http://vipl.ict.ac.cn/uploadfile/upload/2019123017182739.pdf">[PDF]</a>
                &nbsp;
                <a href="https://github.com/mysee1989/PG-CNN">[Code]</a>
            </dt>
            <br />
            
             
            
            <dt>
                <font size="4"><b>Patch-gated cnn for occlusion-aware facial expression recognition</b></font>
            </dt>
            <dt>
                 <strong>Yong Li</strong>, Jiabei Zeng, Shiguang Shan, Xilin Chen
            </dt>
            <dt>
                <i>International Conference on Pattern Recognition (<b>ICPR</b>), 2018 <font color="red"><b>(Oral)</b></font>.</i>
            </dt>
            <dt>
                <a href="http://www.jdl.link/doc/2011/201911019432863571_2018092516364248.pdf">[PDF]</a>
                 &nbsp;
                <a href="https://github.com/mysee1989/PG-CNN">[Code]</a>
            </dt>

            <br />

            <dt>
                <font size="4"><b>Kinnet: Fine-to-coarse deep metric learning for kinship verification</b></font>
            </dt>
            <dt>
                <strong>Yong Li</strong>, Jiabei Zeng, Jie Zhang, Anbo Dai, Meina Kan, Shiguang Shan, Xilin Chen
            </dt>
            <dt>
                <i>Proceedings of the 2017 Workshop on Recognizing Families In the Wild,13-20 <font color="red"><b>(keynote)</b></font>.</i>
            </dt>
            <dt>
                <a href="http://vipl.ict.ac.cn/en/uploadfile/upload/2019082716280770.pdf">[PDF]</a>
            </dt>
            <br />
    
            <dt>
                <font size="4"><b>Context-dependent emotion recognition</b></font>
            </dt>
            <dt>
                Zili Wang, Lingjie Lao, Xiaoya Zhang, <strong>Yong Li*</strong>, Tong Zhang, Zhen Cui
            </dt>
            <dt>
                <i><font color="red"><b>(Best poster in ChinaMM 2022)</b></font>.</i>
            </dt>
            <dt>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320322001997">[PDF]</a>
            </dt>
            <br />

           
        </dl>

        <h2 id="experience">Experience</h2>
        <hr style="margin-top:-16px;margin-bottom:10px;" />

        <dl>
            <!-- <dt>
                <img align="left" width="80" height="80" hspace="10" src="assets/imgs/Megvii-logo.jpeg" />
            </dt> -->
            <dt>
                <font size="4"><b>Baidu Inc. </b>, Beijing, China</font>
            </dt>
            <dt>
                <i>Software R&D Engineer, Jul. 2015 – Jul.2016</i>
            </dt>

            <!-- <dt>
                <img align="left" width="80" height="80" hspace="10" src="assets/imgs/Tusimple-logo.png" />
            </dt> -->
            
        </dl>

        <h2 id="Honors">Honors</h2>
        <hr style="margin-top:-16px;margin-bottom:10px;" />
            <strong>Winner</strong> of the ACM Multimedia 2017 kinship verification Challenge, California, USA <a href="https://vipl.ict.ac.cn/kycg/xsjx/index_1.html">[Link]</a>
            <br />
            <br />
            Zhu Li Yue Hua Scholarship by UCAS in 2019 
            <br />
            <br />
            Zhu Li Yue Hua <strong>Outstanding</strong> Graduate Student Scholarship by UCAS in 2020
            <br />
            <br />
            <strong>Outstanding Doctoral Dissertation</strong> by  China Society of Image and Graphics in 2021 (Total of 10 Per Year in China Mainland) <a href="http://www.csig.org.cn/detail/3362">[Link]</a>

        <h2 id="Services">Services</h2>
        <hr style="margin-top:-16px;margin-bottom:10px;" />
            Reviewer of IJCV / TIP / TMM / TAC / Neurocomputing / PR / CVPR / BMVC / ICPR / ...

        <p>
            <br />
        </p>
    </div>
</body>

</html>
